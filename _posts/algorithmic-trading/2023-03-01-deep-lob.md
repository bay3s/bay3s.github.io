---
layout: post
title:  "Convolutional Networks for High-Frequency Trading"
date:   2023-03-01
category: algorithmic-trading
sub_category: high-frequency trading
---

# Convolutional Neural Networks for High-Frequency Trading
<p>Here we look at developing a large-scale deep learning model for Limit Order Books (LOBs) using convolutional layers to capture the spatial structure and LSTMs to capture longer temporal dependencies. The original model, outlined in <a href="#appendix">[1]</a>, is able to generalize to instruments not included in the training set, indicating the extraction of universal features.</p>
<p>A sensitivity analysis is performed to understand the rationale behind the model's performance and identify the most relevant components for predicting order book dynamics. The key takeaway is the model's feature embeddings are representative of general order book dynamics, which can be translated to other instruments, offering potential for broader applications.</p>
<p><i>Note: I improved the model for which the learning curves are below, but will not be outlining exact improvements in this article as I would like to retain intellectual property.</i></p>

![Learning Curves]({{site.baseurl}}/assets/img/deep-lob-learning-curves.png){: style="width: 100%;" }

# Overview
The evolution in time of LOBs represents a multi-dimensional problem with elements representing,
- Prices
- Order volumes and sizes at multiple levels of the LOB on both sides (bids & asks)
- Orders arranged into different levels based on their prices

# Model Architecture
The neural network architecture that is developed consists of:
- an inception module used to wrap convolutional and pooling layers together
- LSTM for sequential prediction over time-horizons
<br><br>
The intuition behind the architecture is that,
- the inception module helps to infer local interactions over different time horizons.
- feature maps are passed into the LSTM that can capture dynamic temporal behaviour.

# Dataset
<p>The model is trained and tested on the FI-2010 dataset, outperforms all existing state-of-the-art algorithms. It is worth noting that FI-2010 contains only 10 consecutive days of down-sampled pre-normalized data from a low liquidity market.</p>
<p>So, although it serves as a valuable benchmark it may not be sufficient to validate the efficacy of a model in practice.</p>

# Limit Order Book Representation
- Bids have prices $P_b(t)$ and sizes / volumes at each price $V_b(t)$.
- Asks have prices $P_a(t)$ and sizes / volumes at each price $V_a(t)$.
- Orders are sorted into different levels based on submitted prices where L1 represents the first level and so on,
	- $P(t)$ and $V(t)$ are n-vectors, such that n is the number of levels in which orders are segregated. 
	- $p_b^{(1)}$ denotes the highest available price for a buying order (first bid level).
	- $p_a^{(1)}$ denotes the highest lowest price for an ask order (first bid level).

$$
\{p_a^{(i)}(t), v_a^{(i)}(t), p_b^{(i)}(t), v_b^{(i)}(t)\}_{i=1}^{n=10}
$$

# Normalization Approach
- FI-2010 provides 3 different normalized dataset (z-score, min-max, decimal precision normalization),
	- Experiments use the z-score normalized version for FI-2010.
- Proper normalization seems crucial,
	- Financial time-series experience regime shifts, using a static noramlisation scheme is not appropriate in this case.
	- Method used for normalizing is dynamic so that data falls into a reasonable range.
	- Dynamic normalization implicitly casts this into a finite time horizon problem.

# Labelling
- Mid-price is computed after normalizing the data
$$
p_t=\frac{p_a^{(1)}(t)+p_b^{(1)}(t)}{2}
$$
- Labels represent the direction of price changes, unlikely that orders will get executed at the mid-price but should reflect direction of price changes.
- Data is stochastic, comparing $p_t$ and $P_{t+k}$ would result in noisy labeling.
- Solution is to smooth the labels.
- Given a prediction horizon *k*, mean of the next *k* prices is defined as 

$$
m_{+}(t)=\frac{1}{k} \sum_{i=1}^k p_{t+i}
$$

- So, the label for time *t* is given by

$$
l_t=\frac{m_{+}(t)-p_t}{p_t}
$$

- Final label is decided based on a threshold $\alpha$, the minimum percent change that you would want your model to predict / classify,
	- if $l_t \gt \alpha$ then 1
	- if $l_t \lt -\alpha$ then -1
	- anythine else is considered stationary and labeled 0
- It might be worth looking at better labelling schemes, 
	- Median prices instead of the means.
	- Accounting for volumes / liquidity at predicted prices.
	- Volatility / risk should be accounted for as well.

# Experimental Settings
- Same architecture applied to all experiments.
- ADAM is utilised with $\epsilon = 1, lr = 0.01$.
- Learning is stopped when validation accuracy does not improve for 20 more epochs.
- This turns out to be about 100 epochs for FI-2010 and 40 for the LSE dataset.
- Mini-batches of size 32 used for training, smaller batches are chosen since large-batch methods tend to converge to narrow deep minima for trainings but small-batch methods converge to shallow broad minima.

# Does this have potential?
- Due to the development of GPUs, quick predictions can be made which means deep neural nets can be used for HFT.
- DeepLOB seems to learn universal order book features so transfer learning is successful based on experiments run on out-of-sample stocks / instruments.
- However, in practice we likely require a combination of trading signals to time the exact entry and exit points of the trade.
- Accuracy is not great on longer prediction horizons (longer horizon here refers to higher values of *k* used in the labels), but signals are more robust in this case.

# References
<a id="appendix"></a>
- [1] "DeepLOB: Deep Convolutional Neural Networks for Limit Order Books" <a href="https://arxiv.org/abs/1808.03668">https://arxiv.org/abs/1808.03668</a>
